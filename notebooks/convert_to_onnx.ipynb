{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(\"../.env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import onnx\n",
    "import onnxruntime as ort\n",
    "import sentence_transformers\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "from such_toxic.text_classifier import TextClassifier\n",
    "from such_toxic.util import expand, mat_mul, mat_sum, shape, unsqueeze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_uri = f\"runs:/5261022518c9417692ab0d3315ffb9e0/such-toxic\"\n",
    "sentence_transformer_model = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "output_sentence_transformer_model = \"../target/st-all-MiniLM-L6-v2.onnx\"\n",
    "output_such_toxic_model = \"../target/such-toxic.onnx\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(sentence_transformer_model)\n",
    "st_model = AutoModel.from_pretrained(sentence_transformer_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token_embeddings:  torch.Size([1, 6, 384])\n",
      "attention_mask:  torch.Size([1, 6])\n",
      "unsqueezed_attention_mask:  torch.Size([1, 6, 1])\n",
      "input_mask_expanded:  torch.Size([1, 6, 384])\n",
      "token_x_input_mask:  torch.Size([1, 6, 384])\n",
      "s:  torch.Size([1, 384])\n",
      "s:  torch.Size([1, 384])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 384])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output[\n",
    "        0\n",
    "    ]  # First element of model_output contains all token embeddings\n",
    "    print(\"token_embeddings: \", token_embeddings.shape)\n",
    "    print(\"attention_mask: \", attention_mask.shape)\n",
    "\n",
    "    unsqueezed_attention_mask = attention_mask.unsqueeze(-1)\n",
    "    print(\"unsqueezed_attention_mask: \", unsqueezed_attention_mask.shape)\n",
    "\n",
    "    input_mask_expanded = unsqueezed_attention_mask.expand(\n",
    "        token_embeddings.size()\n",
    "    ).float()\n",
    "\n",
    "    print(\"input_mask_expanded: \", input_mask_expanded.shape)\n",
    "    print(\"token_x_input_mask: \", (token_embeddings * input_mask_expanded).shape)\n",
    "\n",
    "    s = torch.sum(token_embeddings * input_mask_expanded, 1)\n",
    "    print(\"s: \", s.shape)\n",
    "    return s / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "\n",
    "st_input = tokenizer(\n",
    "    [\"This is a sample\"],\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "\n",
    "with torch.no_grad():\n",
    "    st_output = st_model(**st_input)\n",
    "\n",
    "st_embedding = mean_pooling(st_output, st_input[\"attention_mask\"])\n",
    "st_embedding = F.normalize(st_embedding, p=2, dim=1)\n",
    "st_embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.onnx.export(\n",
    "    st_model,\n",
    "    (st_input[\"input_ids\"], st_input[\"attention_mask\"]),\n",
    "    output_sentence_transformer_model,\n",
    "    input_names=[\"input_ids\", \"attention_mask\"],\n",
    "    output_names=[\"output\"],\n",
    "    dynamic_axes={\n",
    "        \"input_ids\": {0: \"batch_size\", 1: \"sequence\"},\n",
    "        \"attention_mask\": {0: \"batch_size\", 1: \"sequence\"},\n",
    "    },\n",
    "    do_constant_folding=True,\n",
    "    opset_version=13,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/03/22 15:39:37 WARNING mlflow.pytorch: Stored model version '2.2.1+cu121' does not match installed PyTorch version '2.2.1'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[2.1717e-03, 3.3784e-04, 5.4082e-04, 5.4006e-05, 5.1951e-04, 1.3210e-04]],\n",
       "       grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "such_toxic = mlflow.pytorch.load_model(model_uri, map_location=\"cpu\")\n",
    "such_toxic(st_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.onnx.export(\n",
    "    such_toxic,\n",
    "    st_embedding,\n",
    "    output_such_toxic_model,\n",
    "    input_names=[\"embeddings\"],\n",
    "    output_names=[\"output\"],\n",
    "    dynamic_axes={\"embeddings\": {0: \"batch_size\"}},\n",
    "    do_constant_folding=True,\n",
    "    opset_version=13,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token_embeddings:  [1, 6, 384]\n",
      "unsqueezed_attention_mask:  [1, 6, 1]\n",
      "input_mask_expanded:  [1, 6, 384]\n",
      "token_embeddings_x_input_mask:  [1, 6, 384]\n",
      "masked_sum:  [1]\n"
     ]
    }
   ],
   "source": [
    "def s_mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output[0].tolist()\n",
    "    print(\"token_embeddings: \", shape(token_embeddings))\n",
    "    attention_mask = attention_mask.tolist()\n",
    "    # print(\"attention_mask: \", shape(attention_mask))\n",
    "\n",
    "    unsqueezed_attention_mask = unsqueeze(\n",
    "        attention_mask,\n",
    "        axis=len(shape(attention_mask)),\n",
    "    )\n",
    "    print(\"unsqueezed_attention_mask: \", shape(unsqueezed_attention_mask))\n",
    "\n",
    "    input_mask_expanded = expand(unsqueezed_attention_mask, shape(token_embeddings))\n",
    "    print(\"input_mask_expanded: \", shape(input_mask_expanded))\n",
    "\n",
    "    token_embeddings_x_input_mask = mat_mul(token_embeddings, input_mask_expanded)\n",
    "    print(\"token_embeddings_x_input_mask: \", shape(token_embeddings_x_input_mask))\n",
    "    masked_sum = mat_sum(token_embeddings_x_input_mask, dim=1)\n",
    "    print(\"masked_sum: \", shape(masked_sum))\n",
    "\n",
    "    # s =  torch.sum(token_embeddings * input_mask_expanded, 1)\n",
    "    # print(\"s: \", s.shape)\n",
    "    # return s / torch.clamp(\n",
    "    #     input_mask_expanded.sum(1), min=1e-9\n",
    "    # )\n",
    "\n",
    "    # # # Masked mean with division by non-zero count\n",
    "    # masked_mean = [\n",
    "    #     sum / (sum(mask) + 1e-9) for sum, mask in zip(masked_sum, attention_mask)\n",
    "    # ]\n",
    "    # print(masked_mean)\n",
    "    # return masked_mean\n",
    "\n",
    "\n",
    "st_onnx_model = onnx.load(output_sentence_transformer_model)\n",
    "onnx.checker.check_model(st_onnx_model)\n",
    "\n",
    "stoxic_onnx_model = onnx.load(output_such_toxic_model)\n",
    "onnx.checker.check_model(stoxic_onnx_model)\n",
    "\n",
    "st_session = ort.InferenceSession(output_sentence_transformer_model)\n",
    "st_output = st_session.run(\n",
    "    None,\n",
    "    {\n",
    "        \"input_ids\": st_input[\"input_ids\"].numpy(),\n",
    "        \"attention_mask\": st_input[\"attention_mask\"].numpy(),\n",
    "    },\n",
    ")\n",
    "\n",
    "st_embedding = s_mean_pooling(st_output, st_input[\"attention_mask\"])\n",
    "# print(st_embedding)\n",
    "\n",
    "# st_embedding = F.normalize(st_embedding, p=2, dim=1)\n",
    "\n",
    "\n",
    "# stoxic_session = ort.InferenceSession(output_such_toxic_model)\n",
    "# stoxic_output = stoxic_session.run(None, {\"embeddings\": st_embedding.numpy()})\n",
    "\n",
    "# print(\"Toxic: \", stoxic_output[0][0][0])\n",
    "# print(\"Severe Toxic: \", stoxic_output[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "list(torch.tensor([1, 2, 3]).shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
